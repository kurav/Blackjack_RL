# Playing blackjack using RL - does the house always win?

This project implements a Reinforcement Learning environment for both standard and reverse Blackjack, where agents can learn optimal playing strategies using Q-learning.

## ğŸ® Game Variants

### Standard Blackjack
- Traditional Blackjack rules where the player acts first
- Dealer reveals one card initially
- Standard payout rules (3:2 for blackjack)

### Reverse Blackjack
- A variant where the dealer plays first
- Player has perfect information about dealer's hand
- Changes strategy dynamics significantly
- Useful for studying perfect information scenarios

## ğŸ—ï¸ Project Structure

```
blackjack/
â”œâ”€â”€ agents/           # RL agent implementations
â”œâ”€â”€ saves/           # Training checkpoints and Q-tables
â”œâ”€â”€ visualization_results/  # Training visualization outputs
â”œâ”€â”€ game.py          # Core Blackjack environment
â”œâ”€â”€ reverse_game.py  # Reverse Blackjack variant
â”œâ”€â”€ train.py         # Training utilities
â”œâ”€â”€ train_reverse.py # Reverse Blackjack training
â”œâ”€â”€ play.py          # Interactive gameplay
â””â”€â”€ visualize.py     # Training visualization tools
```

## ğŸ¯ Usage

### Training an Agent

1. Train a standard Blackjack agent:
  ```bash
  python train.py
  ```

2. Train a Reverse Blackjack agent:
  ```bash
  python train_reverse.py
  ```

Training checkpoints and final Q-tables are saved in the `saves/` directory.

### Playing Against Trained Agents

To play against a trained agent:
```bash
python play.py
```

### Visualizing Training Results

To visualize training progress and results:
```bash
python visualize.py
```

## ğŸ² Game Rules

### Standard Blackjack
- Dealer hits on soft 17
- Blackjack pays 3:2
- Double down allowed on any two cards
- Split pairs allowed
- No insurance or surrender

### Reverse Blackjack
- Same rules as standard Blackjack
- Dealer plays first and reveals complete hand
- Player has perfect information
- Strategic decisions are based on known dealer outcome

## ğŸ¤– Reinforcement Learning Details

The project uses Q-learning with the following parameters:
- Learning rate (alpha): 0.1
- Discount factor (gamma): 0.99
- Exploration rate (epsilon): 0.2

The state space includes:
- Player's cards
- Dealer's up card (or complete hand in reverse variant)
- Usable ace status
- Split/double down eligibility

## ğŸ“Š Results

Training results and visualizations are stored in the `visualization_results/` directory, including:
- Learning curves
- Strategy heatmaps
- Win rate analysis
